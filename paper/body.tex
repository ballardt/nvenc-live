
\section{Introduction} \label{intro}

TODO: add citations?

Hardware HEVC encoders have recently appeared on consumer-grade GPUs, opening the door to mass $360\,^{\circ}$ video live streaming. Traditional methods of adaptive bitrate (ABR) streaming, however, do not work on these ultra-high resolution videos because the bitstream will either be too large for continuous playback or of too low a quality to be enjoyable. A body of work on tile-based ABR streaming for UHD video has recently appeared in which the quality requested for each section of a video corresponds roughly to how interesting it is, allowing end-users to view the most important parts of a video in high-quality without stalling. This is straightforward for pre-rendered videos, but is more complicated in a live streaming scenario because each quality requires another encode, introducing some delay in the availability of each new segment. Furthermore, tiling is not supported by existing hardware encoders. One can try to get around this by feeding each tile to the encoder as a separate video and stitching them together afterwards, but this requires cropping once for each desired tile in the image on every frame, then swapping out encoder contexts and encoding each tile separately, introducing considerable overhead.

To combat these challenges, we introduce RATS, a GPU-based HEVC encoding platform to tile, encode, and stitch video at multiple qualities in real-time on a frame-by-frame basis. The source video stream is divided into the desired tile columns and are stacked on top of one another. The rearranged video image is fed to the hardware encoder with slice boundaries specified to at least be the edges of the source video. This process occurs twice for each frame with different bitrate configurations, resulting in high-quality and low-quality outputs. When the video is reconstructed during playback, each tile is an independent region of the frame, allowing us to pick where each tile should appear in the final video and which bitrate it should be.

The rest of this paper is organized as follows. In Section \ref{hevc}, we provide a brief summary of the HEVC encoding process and the NVENC hardware encoder. In Section \ref{rats}, we describe the proposed system in detail. In Section \ref{infra}, we sketch out a full web-based video streaming platform centered around RATS. In Section \ref{eval}, we provide an evaluation of the system. Finally, Section \ref{concl} concludes the work.

\renewcommand{\figurename}{Fig.}
\begin{figure*}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Streaming_scenario_v3.pdf}
	\caption{Adaptive $360\,^{\circ}$ Live Streaming: In this demo we show hardware encoding that allows us to stitch single tiles of the $360\,^{\circ}$ video in different quality levels (see pipeline). The lower part of the figure shows the demo setup in a streaming infrastructure that makes use of HTTP adaptive streaming.}
	\label{fig:pipeline}
\end{figure*}

\section{HEVC and NVENC} \label{hevc}

TODO: add noted citations

HEVC, also known as H.265, is a modern video compression standard capable of dramatically shrinking video file size without sacrificing quality. Encoding an HEVC video is a computationally expensive process, and the most costly operations, such as motion estimation, are not easily parallelized (TODO cite some reference). Instead, encoders divide each frame of the video into independent regions and encode them separately, joining their edges to form the full image during playback. Two versions of this concept, slices and tiles, exist within the HEVC standard. Functionally, they are identical; however, slices are limited to chunking the video into wide strips, while tiles can divide a video into rectangular CTU-based areas of any size. While tiles are supported by most decoders, very few encoders allow for it, all of which are software encoders far too slow to operate in real-time.

Structurally, HEVC bitstreams consist of sequential NAL units, most of which contain a header, generated via a complex implicit syntax, and raw image data. Each slice or tile in the frame is encoded as a single NAL unit. Because tiles are independently-decodable, we can arrange a set of given tiles in any way desired by making just a few modifications to their NAL headers. This process, in which tiles, possibly from multiple separate HEVC bitstreams, are arranged in a particular manner in a single bitstream is known as \textit{stitching}. (TODO: cite that one stitching paper)

NVENC is a state-of-the-art hardware HEVC encoder present on newer Nvidia GPUs. Interactions with NVENC occur through the NVENCODE API, and offer the user a handful of encoding parameters such as target bitrate, slice boundaries, and GOP specifications (TODO cite). It is possible to swap out encoder configurations between frames, allowing one to encode multiple videos simultaneously, but even top-of-the-line consumer GPUs only support a maximum of two contiguous configurations by default. Note that these configurations are not simply encoding parameters, but also contain the previous frame data necessary to encode the next P- or B-frame. These configurations must therefore exist continuously throughout the encoding process. This is a limitation enforced by the driver, and supercomputing GPUs such as the Nvidia Titan V do not have such limits. (TODO cite maybe)

\section{RATS Implementation} \label{rats}
To get around the lack of tiling in NVENC, we divide the RATS pipeline into an encoding component, in which a raw video frame is manipulated and converted into low- and high-quality bitstreams; and a stitching component, in which slices from these bitstreams are converted to tiles and arranged in the desired manner.

Details on the installation and usage of RATS, as well as an overview of the source code, can be found in the git repository \footnote{https://github.com/ballardt/nvenc-live}.

\subsection{Encoding}

NVENC cannot perform tiling, but it is capable of slicing videos in one of several ways based on the \texttt{sliceMode} parameter. When \texttt{sliceMode=3}, each frame is cut into \texttt{sliceModeData}=$n$ equal slices. Slices do not permit vertical boundaries to be specified, but the edges of the image act as natural slice boundaries, allowing us to effectively create tile columns by placing the desired column edges at the image borders. We therefore manipulate the input images by vertically cutting them into $n$ chunks of equal size and stacking these chunks on top of one another, then setting \texttt{sliceModeData=}$k \cdot n$ for some $k \in \mathbb{N}$, so that the slice boundaries align with the top and bottom boundaries of the original image. This process is demonstrated for $n=3$ and $k=1$ in Fig. \ref{fig:pipeline}. Note that $k$ corresponds to the desired number of tile rows, and the CTU height of the image must be divisible by $k$. Similarly, $n$ corresponds to the desired number of tile columns, and the CTU width of the image must be divisible by $n$ to keep stacked columns the same width. We thus crop the input video if necessary, but one could also resize the input video or simply reject it. 

In a YUV420p video, each frame consists of a luminance component followed by two chrominance components. The image transformation is performed on each component via memcpy. For each desired tile column, we begin at the first pixel row and move down, calling memcpy with a length equal to the width of a tile column on each pixel row and placing the contents in a new array, before moving on to the next tile column once we hit the bottom of the image. The result of this process will be an image in which the desired tile columns are stacked on top of one another.

To interact with NVENC, we use the libavcodec API provided by FFmpeg (TODO cite). The high- and low-bitrate configurations are distinct instances of the AVCodecContext struct, which are initialized before the encoding process begins. During the process, each transformed frame is passed to NVENC twice, once for either bitrate, to obtain the low- and high-quality bitstreams.

\subsection{Stitching}

The bitstreams from the encoding step will contain $(k \cdot n)+1$ NAL units per frame, one for each tile and a single SEI at the end, but these will still be arranged in a vertical stack. To pick tiles from either quality and place them in the final image, we must modify several fields in the NAL headers. Table ~\ref{tab:stitch} lists all such fields. Two additional changes are necessary on behalf of emulation prevention bytes and byte alignment. NAL borders are represented by the byte-aligned sequence \texttt{0x000001}. To prevent this sequence from occurring by chance within a NAL, the byte-aligned sequence \texttt{0x03}, referred to as the emulation prevention byte, may be inserted after any byte-aligned \texttt{0x0000} which is not part of a NAL border. The emulation prevention byte does not otherwise impact the NAL parsing process, and decoders will simply skip over it when encountered. 

If the stitching process modifies the number of bits in the NAL header, e.g by inserting a new field or changing an unsigned Exponential Golomb coded value, any emulation prevention bytes appearing after the point of change will likely cease to be byte-aligned. The decoder will then consider these bits to have semantic value, resulting in a corrupt or incorrect header. We must also ensure that the stitching process does not introduce any new \texttt{0x0000} sequences without a trailing emulation prevention byte. To tackle both of these issues at once, we discard all emulation prevention bytes in the original NAL header before making any modifications, then check for byte-aligned \texttt{0x0000} sequences afterwards, inserting emulation prevention bytes as necessary. This procedure is not performed on NAL data because it is not modified, and byte alignment will be performed at the end of the header.

After the last semantic bit in a NAL header or body, byte alignment is performed by appending a \texttt{1} followed by as many \texttt{0}s necessary to complete the byte. As is the case with emulation prevention bytes, if the size of the NAL header changes during modification, we will likely have an incorrect number of trailing \texttt{0}s. To redo the byte alignment, we find the last \texttt{1} in the original NAL component and remove it and everything after it, effectively undoing the previous byte alignment. We then perform byte alignment after our modifications have been made.

The stitching process uses the Boost C++ dynamic\_bitstream API, which allows one to read bytes into a vector of bits, manipulate these bits, then convert the bit vector back into raw bytes. The high- and low-bitrate configurations are identical except for the specified bitrate, so the output from NVENC is highly predictable. Navigating to the right spot in the bitstream and making our changes is thus a straightforward process.

\setcounter{figure}{1}
\begin{table}
	\label{tab:stitch}
	\begin{tabularx}{\columnwidth}{llr}
		\toprule
		NAL Type & Field & Value \\
		\midrule
		%\multirow{2}{*}[-.3em]{SPS} & \texttt{pic\_width\_in\_luma\_samples} & & \\ 
		SPS & \texttt{pic\_width\_in\_luma\_samples} & \\
		 & \texttt{pic\_height\_in\_luma\_samples} &  \\
		\midrule
		PPS & \texttt{tiles\_enabled\_flag} & \texttt{1}  \\
		& \texttt{num\_tile\_columns\_minus1} &  \\ 
		& \texttt{num\_tile\_rows\_minus1} &  \\
		& \texttt{uniform\_spacing\_flag} & \texttt{1}  \\
		\midrule
		I-frame, & \texttt{first\_slice\_segment\_in\_pic\_flag} & \\
		P-frame & \texttt{slice\_segment\_address} &  \\
		& \texttt{num\_entry\_point\_offsets} & \texttt{1}  \\
		\bottomrule
	\end{tabularx}
	\caption{Table containing all NAL header fields requiring modification. Required literal bit values are provided when necessary. Note that \texttt{num\_entry\_point\_offsets} is an Exponential Golomb-coded number, so a literal bit value of \texttt{1} corresponds to a logical value of 0.}
\end{table}
\renewcommand{\figurename}{Fig.}
\setcounter{figure}{1}

\subsection{Hardware Limitations}

In this work, we use the Nvidia GTX 1080Ti, a powerful consumer-grade GPU. As demonstrated in Section~\ref{eval}, this GPU meets our real-time speed requirements; however, there are two limitations that we must consider.

First, the maximum input image size is $8192\times8192$ pixels. Because we stack tile columns before encoding, this limit is quickly reached. To get around this, we divide the image into sub-images whose stacked columns do not exceed $8192$ pixels, and encode each of these sub-images independently at either quality. More tile columns thus require additional encodes, but NVENC is fast enough to keep the total encoding time well below our target.

We must make a few small changes to the stitching process to handle sub-images. Note that the position of each tile within a sub-image may change in the final image. For example, consider a video with 6 tile columns. From left to right, if the first 4 columns constitute one sub-image and the final 2 columns constitute another, the CTU offsets of all tiles except those in the first row of the first sub-image will change, and the number of required bits to record the offset will change as well. Furthermore, the first tile in the second sub-image will not contain a CTU offset, so this value must be inserted.

Second, as detailed in Section 2, the GPU driver does not allow more than 2 simultaneous context configurations during the encoding process, which would normally prevent the use of sub-images. Fortunately, it is possible to remove the 2 context limit by sending a particular bytestream to the GPU. A script to do so is provided in nvidia-patch \footnote{https://github.com/keylase/nvidia-patch}, which we used for this demo.

\begin{figure}[t]
	\includegraphics[width=\columnwidth]{figures/times_v1.pdf}
	\caption{Encoding time for a 30s video across tile configurations. The tile qualities alternate between high and low in a checkerboard pattern. For each configuration that has an odd number of tiles, we set the extra tile to be low quality. Note the lower encoding time for 8 columns vs 7 columns which is due to better alignment.}
	\label{fig:time}
\end{figure}

\section{RATS in an Infrastructure} \label{infra}

The main contribution of RATS to the state-of-the-art is the efficient and accessible
tiling of UHD videos on a single computer equipped with a consumer-grade GPU. We circumvent certain limitations of the GPU and are thereby able to generate videos compliant with the advanced tiling scheme of HEVC.

In the demo, we present
a variation of our solution that stitches tiles of various
qualities together before delivery to an end-system.
This is not the only possible approach, and is actually somewhat more complex
than eliminating the stitching component and simply creating distinct video streams for each tile instead. 
DASH provides the extension DASH-SRD (spatial relationship description), which
allows a client to access tile streams individually, synchronize them, and make
independent quality choices for each (TODO cite). Providing the appropriate data and
metadata for this approach is actually a simplification of the one we
demonstrate in this paper.

The remaining infrastructure used to deliver the encoded video via HTTP adaptive streaming is entirely comprised of open-source software, including Nginx\footnote{https://github.com/nginx/nginx}, Kaltura nginx-vod-module\footnote{https://github.com/kaltura/nginx-vod-module} and MediaElement.js\footnote{https://www.mediaelementjs.com/}.

%\begin{itemize}
%\item NGinx: available from \url{https://github.com/nginx/nginx.git}
%\item Kaltura nginx-vod-module: available from \url{https://github.com/kaltura/nginx-vod-module.git}
%\item MediaElement.js: available from \url{https://www.mediaelementjs.com/}
%\end{itemize}


\section{Evaluation} \label{eval}
To evaluate RATS, we analyze its performance across multiple tile configurations. In particular, we examine the encoding speed, output file size, and the quality of the final image. Evaluations were performed with an NVIDIA GTX 1080 Ti GPU on Ubuntu 16.04. The source video used was 30 seconds long with a resolution of $3940\times2048$ pixels. The source video was originally encoded with Apple PRORES, and FFMpeg was used to convert it to a series of raw YUV images.

Examining Figs. \ref{fig:time} and \ref{fig:size}, we see that the number of tile columns is the primary factor affecting encoder performance. Intuitively, this makes sense; the number of columns dictates the number of sub-images necessary, each of which requires two more NVENC encodes per frame and multiple stitching operations. Furthermore, stacking each tile column requires additional transformations of the source image. In contrast, tile rows are simply required to divide evenly along CTU borders, incurring minor computation costs only when they are misaligned and the source image must be cropped.

In Fig. \ref{fig:time}, there are several notable jumps which highlight various components of the RATS pipeline. The first of these occurs between 1 and 2 tile columns as the source image starts transforming and the stitcher begins to make more significant bitstream modifications. When there is only one tile column, tiles are simply rows across the whole image, and the stitching process consists mostly of picking the quality for each row without rearranging any tiles. Another such jump occurs between 4 and 5 tile columns as sub-images are introduced. Interestingly, the spike at the 7 column originates entirely within NVENC. This is likely due to the fact that the CTU width of the uncropped source image is not divisible by 7, but all other values tested are. TODO: better explanation

TODO: discuss fig. \ref{fig:size}
TODO: do other jumps originate entirely in NVENC?

%\begin{figure}[t]
%	% \includegraphics[width=\columnwidth]{figures/hevc_eval_qual.png}
%	\caption{Encoding speed vs. tile quality for $4\times4$ tiles. All tiles on the left are of the high bitrate, while those on the right are of the low bitrate.}
%\end{figure}





\begin{figure}[t]
	\includegraphics[width=\columnwidth]{figures/sizes_v1.pdf}
	\caption{Encoding output size in MB for a 30s video across tile configurations. The tile qualities alternate between high and low in a checkerboard pattern. }
	\label{fig:size}
\end{figure}

\section{Conclusion} \label{concl}

In this work, we demonstrated an adaptive live video platform capable of encoding tiled $360\,^{\circ}$ videos at multiple bitrates using an accessible consumer-grade GPU. By working closely with NVENCODE and the HEVC standard, we overcame limitations that had previously rendered hardware encoders useless in generating adaptive tile-based video. Furthermore, we have made our code open source and provided a demonstration of an HTTP video streaming service based on RATS.

\begin{figure}[t]
	\includegraphics[width=\columnwidth]{figures/times_size_combined_v1.pdf}
	\caption{JUST FOR US. A combined view of the encoding time and the resulting output size.}
\end{figure}
